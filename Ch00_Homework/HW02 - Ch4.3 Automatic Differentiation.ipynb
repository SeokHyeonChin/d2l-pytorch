{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeokHyeon Chin, 2016320179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we train models, updating them successively so that they get better and better as they see more and more data. Usually, getting better means minimizing a loss function.\n",
    "With neural networks, we typically choose loss functions that are differentiable with respect to our parameters.\n",
    "The `autograd` package expedites this work by automatically calculating derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of differentiating the mapping $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to the column vector $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.arange(4, dtype=torch.float32).reshape((4, 1)), requires_grad=True)\n",
    "print(x) # creates (4X1) vector which elements are Real number\n",
    "\n",
    "# Discussion: What if not declare requires_grad or compute as False?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute the gradient of y with respect to x, we will need a place to store it. We can tell a tensor that we plan to store a gradient by the ``requires_grad=True`` keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compute y and PyTorch will generate a computation graph on the fly. Autograd is reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.variable.Variable"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable # For a tensor to be ‚Äúrecordable‚Äù, it must be wrapped with torch.autograd.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*torch.mm(x.t(),x) # y is a scalar\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # can automatically find the gradient with this fucntion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Variable has an associated grad_fn, which is the torch.autograd.Function that is used to compute the backward step. For inputs it is None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([[ 0.],\n",
      "        [ 4.],\n",
      "        [ 8.],\n",
      "        [12.]])\n",
      "x.grad_fn: None\n",
      "y.grad_fn: <MulBackward0 object at 0x00000250B45C9488>\n"
     ]
    }
   ],
   "source": [
    "print(\"x.grad:\", x.grad) # y_gradient = 4x, which makes (0,4,8,12)\n",
    "print(\"x.grad_fn:\", x.grad_fn)\n",
    "print(\"y.grad_fn:\", y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function  ùë¶=2ùê±‚ä§ùê±  with respect to  ùê±  should be  4ùê± . Now let's verify that the gradient produced is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[ 0.],\n",
      "        [ 4.],\n",
      "        [ 8.],\n",
      "        [12.]])\n"
     ]
    }
   ],
   "source": [
    "print((x.grad - 4*x).norm().item() == 0)\n",
    "print(x.grad) # Note that gradient of y is shown by x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((x.grad - 4*x)) # Discussion: What does 'grad_fn=<SubBackward0>', 'grad_fn=<NormBackward0>' mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((x.grad - 4*x).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.arange(4, dtype=torch.float32).reshape((4, 1)))\n",
    "y = 2*torch.mm(x.t(),x)\n",
    "print(y)\n",
    "#y.backward()\n",
    "\n",
    "#RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.arange(4, dtype=torch.float32).reshape((4, 1)), requires_grad=False)\n",
    "y = 2*torch.mm(x.t(),x)\n",
    "print(y)\n",
    "#y.backward() \n",
    "\n",
    "#RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without `requires_grad=True` keyword or if we compute this keyword as `False`, `Autograd` does not work. If we use `backward` function with this state, `RuntimeError` as above occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode and Evaluation Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model` will change the running mode to the evaluation mode on calling `model.eval()` or to the training mode on calling `model.train()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Gradient of Python Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the computational graph of the function contains Python's control flow, we can still find the gradient of a variable. Consider the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm().item() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum().item() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of iterations of the while loop and the execution of the conditional statement (if then else) depend on the value of `a`. To compute gradients, we need to `record` the calculation, and then call the `backward` function to calculate the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1136])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(size=(1,)) # create random variable\n",
    "print(a)\n",
    "a.requires_grad=True # to use Autograd\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad: tensor([16384.])\n",
      "a.grad_fn: None\n",
      "d.grad_fn: <MulBackward0 object at 0x00000250B45EB788>\n"
     ]
    }
   ],
   "source": [
    "print(\"a.grad:\", a.grad)\n",
    "print(\"a.grad_fn:\", a.grad_fn)\n",
    "print(\"d.grad_fn:\", d.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head gradients and the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when we call the backward method, e.g. `y.backward()`, where\n",
    "`y` is a function of `x` we are just interested in the derivative of\n",
    "`y` with respect to `x`. Mathematicians write this as\n",
    "$\\frac{dy(x)}{dx}$. At other times, we may be interested in the\n",
    "gradient of `z` with respect to `x`, where `z` is a function of `y`,\n",
    "which in turn, is a function of `x`. That is, we are interested in\n",
    "$\\frac{d}{dx} z(y(x))$. Recall that by the chain rule\n",
    "\n",
    "$$\\frac{d}{dx} z(y(x)) = \\frac{dz(y)}{dy} \\frac{dy(x)}{dx}.$$\n",
    "\n",
    "So, when ``y`` is part of a larger function ``z`` and we want ``x.grad`` to store $\\frac{dz}{dx}$, we can pass in the *head gradient* $\\frac{dz}{dy}$ as an input to ``backward()``. The default argument is ``torch.ones_like(y)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [4.0000],\n",
      "        [0.8000],\n",
      "        [0.1200]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.tensor([[0.],[1.],[2.],[3.]]), requires_grad=True)\n",
    "y = x * 2\n",
    "z = y * x\n",
    "\n",
    "head_gradient = torch.tensor([[10], [1.], [.1], [.01]]) # Discussion: meaning of this?\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.],\n",
      "        [ 4.],\n",
      "        [ 8.],\n",
      "        [12.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.tensor([[0.],[1.],[2.],[3.]]), requires_grad=True)\n",
    "y = x * 2\n",
    "z = y * x\n",
    "\n",
    "head_gradient = torch.ones_like(y) # default usage (1,1,1,1)\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* PyTorch provides an `autograd` package to automate the derivation process.\n",
    "* PyTorch's `autograd` package can be used to derive general imperative programs.\n",
    "* The running modes of PyTorch include the training mode and the evaluation mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "* To use `autograd` function `.backward()`, you must write `requires_grad=True`. Otherwise, `RuntimeError` will occur.\n",
    "* What does `grad_fn=<SubBackward0>`, `grad_fn=<NormBackward0>` mean? It seems that once you start to store a gradient by the `requires_grad=True` keyword, Pytorch tracks what you are doing with that variable and record as shown.\n",
    "* `head_gradient = torch.tensor([[10], [1.], [.1], [.01]])` reason of ths usage is described below:\n",
    "* Some matrix represents the gradient of f(X) with respect to X.\n",
    "Suppose a PyTorch gradient enabled tensors X as:\n",
    "X = [x1, x2, ‚Ä¶.. xn] (Let this be the weights of some machine learning model)\n",
    "X undergoes some operations to form a vector Y\n",
    "Y = f(X) = [y1, y2, ‚Ä¶. ym]\n",
    "Y is then used to calculate a scalar loss l. Suppose a vector v happens to be the gradient of the scalar loss l with respect the vector Y as follows: \n",
    "Image for post\n",
    "The vector v is called the grad_tensor and passed to the backward() function as an argument\n",
    "To get the gradient of the loss l with respect to the weights X the Jacobian matrix J is vector-multiplied with the vector v\n",
    "Image for post\n",
    "This method of calculating the Jacobian matrix and multiplying it with a vector v enables the possibility for PyTorch to feed external gradients with ease for even the non-scalar outputs.\n",
    "* Source: https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
